{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f0b0bf-a601-4281-9414-ca18d47d9f70",
   "metadata": {},
   "source": [
    "**Google AI Research Code Competition: American Sign Language Fingerspelling Recognition**\n",
    "- link: <https://www.kaggle.com/competitions/asl-fingerspelling>\n",
    "\n",
    "**Notebook Notations**\n",
    "- [calv, kaggle]: Original code by Calv on Kaggle;\n",
    "- [calv, vertex ai]: Adapted code by Calv on Vertex AI, Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c842ad3a-b522-4882-a93b-1e26261115de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [1]\n",
    "# %pip install seaborn\n",
    "# %pip install leven\n",
    "# %pip install pyarrow\n",
    "# %pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ae45dd-bd56-4c9b-984f-b60116dd9fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from tensorflow_addons) (23.1)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from tensorflow_addons) (2.13.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: leven in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (1.0.4)\n",
      "Requirement already satisfied: six in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from leven) (1.16.0)\n",
      "Requirement already satisfied: nose in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from leven) (1.3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# [calv, vertex ai] [1]\n",
    "%pip install tensorflow_addons\n",
    "%pip install leven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3385eb-d53d-4fb4-b551-84648eba3746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 04:18:06.578817: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-05 04:18:07.619482: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-05 04:18:07.619595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-05 04:18:07.619606: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 2.11.0\n",
      "Python Version: 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# [calv, kaggle] [2]\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from leven import levenshtein\n",
    "\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import sys\n",
    "import sklearn\n",
    "import time\n",
    "import json\n",
    "\n",
    "# TQDM Progress Bar With Pandas Apply Function\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f'Tensorflow Version {tf.__version__}')\n",
    "print(f'Python Version: {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca10c89b-ee3f-4e86-9b39-ef714f618db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [3]\n",
    "# Read Character to Ordinal Encoding Mapping\n",
    "# with open('/kaggle/input/asl-fingerspelling/character_to_prediction_index.json') as json_file:\n",
    "#     CHAR2ORD = json.load(json_file)\n",
    "    \n",
    "# Ordinal to Character Mapping\n",
    "# ORD2CHAR = {j:i for i, j in CHAR2ORD.items()}\n",
    "    \n",
    "# Character to Ordinal Encoding Mapping   \n",
    "# display(pd.Series(CHAR2ORD).to_frame('Ordinal Encoding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc730fb9-ab3b-41e4-ac07-ecc88108878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, vertex ai] [3]\n",
    "# Read Character to Ordinal Encoding Mapping\n",
    "# with open('/kaggle/input/asl-fingerspelling/character_to_prediction_index.json') as json_file:\n",
    "#     CHAR2ORD = json.load(json_file)\n",
    "\n",
    "CHAR2ORD = {\n",
    "    \" \":0, \"!\":1, \"#\":2, \"$\":3, \"%\":4, \"&\":5, \"'\":6, \"(\":7, \")\":8, \"*\":9,\n",
    "    \"+\":10, \",\":11, \"-\":12, \".\":13, \"\\/\":14, \"0\":15, \"1\":16, \"2\":17, \"3\":18, \"4\":19,\n",
    "    \"5\":20, \"6\":21, \"7\":22, \"8\":23, \"9\":24, \":\":25, \";\":26, \"=\":27, \"?\":28, \"@\":29,\n",
    "    \"[\":30, \"_\":31, \"a\":32, \"b\":33, \"c\":34, \"d\":35, \"e\":36, \"f\":37, \"g\":38, \"h\":39,\n",
    "    \"i\":40, \"j\":41, \"k\":42, \"l\":43, \"m\":44, \"n\":45, \"o\":46, \"p\":47, \"q\":48, \"r\":49,\n",
    "    \"s\":50, \"t\":51, \"u\":52, \"v\":53, \"w\":54, \"x\":55, \"y\":56, \"z\":57, \"~\":58\n",
    "}\n",
    "\n",
    "# Ordinal to Character Mapping\n",
    "ORD2CHAR = {j:i for i, j in CHAR2ORD.items()}\n",
    "    \n",
    "# Character to Ordinal Encoding Mapping   \n",
    "# display(pd.Series(CHAR2ORD).to_frame('Ordinal Encoding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb50400b-b458-4a0b-b1e4-45caaec31383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [4]\n",
    "# If Notebook Is Run By Committing or In Interactive Mode For Development\n",
    "# IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n",
    "IS_INTERACTIVE = 0\n",
    "# Verbose Setting during training\n",
    "VERBOSE = 1 if IS_INTERACTIVE else 2\n",
    "# Global Random Seed\n",
    "SEED = 42\n",
    "# Number of Frames to resize recording to\n",
    "N_TARGET_FRAMES = 128\n",
    "# Global debug flag, takes subset of train\n",
    "DEBUG = False\n",
    "# Number of Unique Characters To Predict + Pad Token + SOS Token + EOS Token\n",
    "N_UNIQUE_CHARACTERS0 = len(CHAR2ORD)\n",
    "N_UNIQUE_CHARACTERS = len(CHAR2ORD) + 1 + 1 + 1\n",
    "PAD_TOKEN = len(CHAR2ORD) # Padding\n",
    "SOS_TOKEN = len(CHAR2ORD) + 1 # Start Of Sentence\n",
    "EOS_TOKEN = len(CHAR2ORD) + 2 # End Of Sentence\n",
    "# Whether to use 10% of data for validation\n",
    "USE_VAL = False\n",
    "# Batch Size\n",
    "BATCH_SIZE = 64\n",
    "# Number of Epochs to Train for\n",
    "# N_EPOCHS = 2 if IS_INTERACTIVE else 100\n",
    "N_EPOCHS = 2 if IS_INTERACTIVE else 200\n",
    "# Number of Warmup Epochs in Learning Rate Scheduler\n",
    "N_WARMUP_EPOCHS = 10\n",
    "# Maximum Learning Rate\n",
    "LR_MAX = 1e-3\n",
    "# Weight Decay Ratio as Ratio of Learning Rate\n",
    "WD_RATIO = 0.05\n",
    "# Length of Phrase + EOS Token\n",
    "MAX_PHRASE_LENGTH = 31 + 1\n",
    "# Whether to Train The model\n",
    "TRAIN_MODEL = True\n",
    "# Whether to Load Pretrained Weights\n",
    "LOAD_WEIGHTS = False\n",
    "# Learning Rate Warmup Method [log, exp]\n",
    "WARMUP_METHOD = 'exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f02b0c-83aa-4c8a-b4a2-d5d632a2dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [5]\n",
    "# MatplotLib Global Settings\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams['xtick.labelsize'] = 16\n",
    "mpl.rcParams['ytick.labelsize'] = 16\n",
    "mpl.rcParams['axes.labelsize'] = 18\n",
    "mpl.rcParams['axes.titlesize'] = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea98349-e6c5-4f08-8947-17cfa6837aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [6]\n",
    "# Read Train DataFrame\n",
    "# if DEBUG:\n",
    "#     train = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv').head(5000)\n",
    "# else:\n",
    "#     train = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n",
    "    \n",
    "# Set Train Indexed By sequence_id\n",
    "# train_sequence_id = train.set_index('sequence_id')\n",
    "\n",
    "# Number of Train Samples\n",
    "# N_SAMPLES = len(train)\n",
    "# print(f'N_SAMPLES: {N_SAMPLES}')\n",
    "\n",
    "# display(train.info())\n",
    "# display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30425ff-e074-4a5b-a338-21e419c93f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, vertex ai] [6]\n",
    "# Read Train DataFrame\n",
    "# if DEBUG:\n",
    "#     train = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv').head(5000)\n",
    "# else:\n",
    "#     train = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n",
    "\n",
    "# Read Train DataFrame\n",
    "train = pd.read_csv(\"gs://calvinai-aslfr/train.csv\")\n",
    "\n",
    "# Set Train Indexed By sequence_id\n",
    "train_sequence_id = train.set_index('sequence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8d40ea-d4a5-4010-8f28-a2af84160873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [7]\n",
    "# Get complete file path to file\n",
    "# def get_file_path(path):\n",
    "#     return f'/kaggle/input/asl-fingerspelling/{path}'\n",
    "\n",
    "# train['file_path'] = train['path'].apply(get_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a5a5d7e-8765-4ed2-975f-429976ffb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [8]\n",
    "# Unique Parquet Files\n",
    "# INFERENCE_FILE_PATHS = pd.Series(\n",
    "#     glob.glob('/kaggle/input/aslfr-preprocessing-dataset/train_landmark_subsets/*')\n",
    "# )\n",
    "\n",
    "# print(f'Found {len(INFERENCE_FILE_PATHS)} Inference Pickle Files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a572179-7079-4e71-951a-4d25fbadc5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 Inference Pickle Files\n"
     ]
    }
   ],
   "source": [
    "# [calv, vertex ai] [8]\n",
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(\"calvinai-aslfr\")\n",
    "\n",
    "# Unique Parquet Files\n",
    "INFERENCE_FILE_PATHS = pd.Series(\n",
    "    # glob.glob('/kaggle/input/aslfr-preprocessing-dataset/train_landmark_subsets/*')\n",
    "    bucket.list_blobs(prefix=\"train_landmark_subsets\")\n",
    ")\n",
    "\n",
    "print(f'Found {len(INFERENCE_FILE_PATHS)} Inference Pickle Files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec32a635-e3e2-48b6-8b03-e8c613acfbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [9]\n",
    "# Train/Validation\n",
    "# if USE_VAL:\n",
    "    # TRAIN\n",
    "    # X_train = np.load('/kaggle/input/aslfr-preprocessing-dataset/X_train.npy')\n",
    "    # y_train = np.load('/kaggle/input/aslfr-preprocessing-dataset/y_train.npy')[:, :MAX_PHRASE_LENGTH]\n",
    "    # N_TRAIN_SAMPLES = len(X_train)\n",
    "    # VAL\n",
    "    # X_val = np.load('/kaggle/input/aslfr-preprocessing-dataset/X_val.npy')\n",
    "    # y_val = np.load('/kaggle/input/aslfr-preprocessing-dataset/y_val.npy')[:, :MAX_PHRASE_LENGTH]\n",
    "    # N_VAL_SAMPLES = len(X_val)\n",
    "    # Shapes\n",
    "    # print(f'X_train shape: {X_train.shape}, X_val shape: {X_val.shape}')\n",
    "# Train On All Data\n",
    "# else:\n",
    "    # TRAIN\n",
    "    # X_train = np.load('/kaggle/input/aslfr-preprocessing-dataset/X.npy')\n",
    "    # y_train = np.load('/kaggle/input/aslfr-preprocessing-dataset/y.npy')[:, :MAX_PHRASE_LENGTH]\n",
    "    # N_TRAIN_SAMPLES = len(X_train)\n",
    "    # print(f'X_train shape: {X_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442ee66-d6a8-46c2-935a-ed33502c14b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, vertex ai] [9]\n",
    "import gcsfs\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "# Train On All Data\n",
    "# TRAIN\n",
    "with fs.open(\"gs://calvinai-aslfr/X.npy\", \"rb\") as f:\n",
    "    X_train = np.load(f)\n",
    "    \n",
    "with fs.open(\"gs://calvinai-aslfr/y.npy\", \"rb\") as f:\n",
    "    y_train = np.load(f)[:, :MAX_PHRASE_LENGTH]\n",
    "\n",
    "# N_TRAIN_SAMPLES = len(X_train)\n",
    "# print(f'X_train shape: {X_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912f225-d857-4cdf-be0c-60f69ed2e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [10]\n",
    "# Example Batch For Debugging\n",
    "N_EXAMPLE_BATCH_SAMPLES = 1024\n",
    "N_EXAMPLE_BATCH_SAMPLES_SMALL = 32\n",
    "# Example Batch\n",
    "X_batch = {\n",
    "    'frames': np.copy(X_train[:N_EXAMPLE_BATCH_SAMPLES]),\n",
    "    'phrase': np.copy(y_train[:N_EXAMPLE_BATCH_SAMPLES]),\n",
    "    # 'phrase_type': np.copy(y_phrase_type_train[:N_EXAMPLE_BATCH_SAMPLES]),\n",
    "}\n",
    "y_batch = np.copy(y_train[:N_EXAMPLE_BATCH_SAMPLES])\n",
    "# Small Example Batch\n",
    "X_batch_small = {\n",
    "    'frames': np.copy(X_train[:N_EXAMPLE_BATCH_SAMPLES_SMALL]),\n",
    "    'phrase': np.copy(y_train[:N_EXAMPLE_BATCH_SAMPLES_SMALL]),\n",
    "    # 'phrase_type': np.copy(y_phrase_type_train[:N_EXAMPLE_BATCH_SAMPLES_SMALL]),\n",
    "}\n",
    "y_batch_small = np.copy(y_train[:N_EXAMPLE_BATCH_SAMPLES_SMALL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae29dc2-c888-4243-8df5-3ec599e1b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [11]\n",
    "# Read First Parquet File\n",
    "# example_parquet_df = pd.read_parquet(train['file_path'][0])\n",
    "example_parquet_df = pd.read_parquet(INFERENCE_FILE_PATHS[0])\n",
    "\n",
    "# Each parquet file contains 1000 recordings\n",
    "print(f'# Unique Recording: {example_parquet_df.index.nunique()}')\n",
    "# Display DataFrame layout\n",
    "display(example_parquet_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3f654-32a5-4b99-8eea-4101ae9db5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [12]\n",
    "# Get indices in original dataframe\n",
    "def get_idxs(df, words_pos, words_neg=[], ret_names=True, idxs_pos=None):\n",
    "    idxs = []\n",
    "    names = []\n",
    "    for w in words_pos:\n",
    "        for col_idx, col in enumerate(example_parquet_df.columns):\n",
    "            # Exclude Non Landmark Columns\n",
    "            if col in ['frame']:\n",
    "                continue\n",
    "                \n",
    "            col_idx = int(col.split('_')[-1])\n",
    "            # Check if column name contains all words\n",
    "            if (w in col) and (idxs_pos is None or col_idx in idxs_pos) and all([w not in col for w in words_neg]):\n",
    "                idxs.append(col_idx)\n",
    "                names.append(col)\n",
    "    # Convert to Numpy arrays\n",
    "    idxs = np.array(idxs)\n",
    "    names = np.array(names)\n",
    "    # Returns either both column indices and names\n",
    "    if ret_names:\n",
    "        return idxs, names\n",
    "    # Or only columns indices\n",
    "    else:\n",
    "        return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b400d74-b4cc-4824-a55e-55426f40bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [13]\n",
    "# Lips Landmark Face Ids\n",
    "# LIPS_LANDMARK_IDXS = np.array([\n",
    "#     61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "#     291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "#     78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "#     95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "# ])\n",
    "\n",
    "# Landmark Indices for Left/Right hand without z axis in raw data\n",
    "LEFT_HAND_IDXS0, LEFT_HAND_NAMES0 = get_idxs(example_parquet_df, ['left_hand'], ['z'])\n",
    "RIGHT_HAND_IDXS0, RIGHT_HAND_NAMES0 = get_idxs(example_parquet_df, ['right_hand'], ['z'])\n",
    "# LIPS_IDXS0, LIPS_NAMES0 = get_idxs(example_parquet_df, ['face'], ['z'], idxs_pos=LIPS_LANDMARK_IDXS)\n",
    "COLUMNS0 = np.concatenate(\n",
    "    # (LEFT_HAND_NAMES0, RIGHT_HAND_NAMES0, LIPS_NAMES0)\n",
    "    (LEFT_HAND_NAMES0, RIGHT_HAND_NAMES0)\n",
    ")\n",
    "N_COLS0 = len(COLUMNS0)\n",
    "# Only X/Y axes are used\n",
    "N_DIMS0 = 2\n",
    "\n",
    "print(f'N_COLS0: {N_COLS0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e2ed7-bb34-45d6-a998-58a084e25697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [14]\n",
    "# Landmark Indices in subset of dataframe with only COLUMNS selected\n",
    "LEFT_HAND_IDXS = np.argwhere(np.isin(COLUMNS0, LEFT_HAND_NAMES0)).squeeze()\n",
    "RIGHT_HAND_IDXS = np.argwhere(np.isin(COLUMNS0, RIGHT_HAND_NAMES0)).squeeze()\n",
    "# LIPS_IDXS = np.argwhere(np.isin(COLUMNS0, LIPS_NAMES0)).squeeze()\n",
    "HAND_IDXS = np.concatenate((LEFT_HAND_IDXS, RIGHT_HAND_IDXS), axis=0)\n",
    "N_COLS = N_COLS0\n",
    "# Only X/Y axes are used\n",
    "N_DIMS = 2\n",
    "\n",
    "print(f'N_COLS: {N_COLS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff3872-ad07-469c-908b-40c2e0fe8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [15]\n",
    "# Indices in processed data by axes with only dominant hand\n",
    "HAND_X_IDXS = np.array(\n",
    "    [idx for idx, name in enumerate(LEFT_HAND_NAMES0) if 'x' in name]\n",
    ").squeeze()\n",
    "HAND_Y_IDXS = np.array(\n",
    "    [idx for idx, name in enumerate(LEFT_HAND_NAMES0) if 'y' in name]\n",
    ").squeeze()\n",
    "# Names in processed data by axes\n",
    "HAND_X_NAMES = LEFT_HAND_NAMES0[HAND_X_IDXS]\n",
    "HAND_Y_NAMES = LEFT_HAND_NAMES0[HAND_Y_IDXS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cba10b-bc1d-48e1-85e8-17f8d56f0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [16]\n",
    "# Mean/Standard Deviations of data used for normalizing\n",
    "MEANS = np.load(\n",
    "    # '/kaggle/input/aslfr-preprocessing-dataset/MEANS.npy'\n",
    "    \"gs://calvinai-aslfr/MEANS.npy\"\n",
    ").reshape(-1)\n",
    "STDS = np.load(\n",
    "    # '/kaggle/input/aslfr-preprocessing-dataset/STDS.npy'\n",
    "    \"gs://calvinai-aslfr/STDS.npy\"\n",
    ").reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef944ba1-0fcd-48f5-a34d-6004abe50187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [17]\n",
    "\"\"\"\n",
    "    Tensorflow layer to process data in TFLite\n",
    "    Data needs to be processed in the model itself, so we can not use Python\n",
    "\"\"\" \n",
    "class PreprocessLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PreprocessLayer, self).__init__()\n",
    "        self.normalisation_correction = tf.constant(\n",
    "            # Add 0.50 to x coordinates of left hand (original right hand) and subtract 0.50 of right hand (original left hand)\n",
    "            [0.50 if 'x' in name else 0.00 for name in LEFT_HAND_NAMES0],\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "    \n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec(shape=[None, N_COLS0], dtype=tf.float32), ),\n",
    "    )\n",
    "    def call(self, data0, resize=True):\n",
    "        # Fill NaN Values With 0\n",
    "        data = tf.where(tf.math.is_nan(data0), 0.0, data0)\n",
    "        \n",
    "        # Hacky\n",
    "        data = data[None]\n",
    "        \n",
    "        # Empty Hand Frame Filtering\n",
    "        hands = tf.slice(data, [0, 0, 0], [-1, -1, 84])\n",
    "        hands = tf.abs(hands)\n",
    "        mask = tf.reduce_sum(hands, axis=2)\n",
    "        mask = tf.not_equal(mask, 0)\n",
    "        data = data[mask][None]\n",
    "        \n",
    "        # Pad Zeros\n",
    "        N_FRAMES = len(data[0])\n",
    "        if N_FRAMES < N_TARGET_FRAMES:\n",
    "            data = tf.concat((\n",
    "                data,\n",
    "                tf.zeros([1, N_TARGET_FRAMES - N_FRAMES, N_COLS], dtype=tf.float32)\n",
    "            ), axis=1)\n",
    "        # Downsample\n",
    "        data = tf.image.resize(\n",
    "            data,\n",
    "            [1, N_TARGET_FRAMES],\n",
    "            method=tf.image.ResizeMethod.BILINEAR,\n",
    "        )\n",
    "        \n",
    "        # Squeeze Batch Dimension\n",
    "        data = tf.squeeze(data, axis=[0])\n",
    "        \n",
    "        return data\n",
    "    \n",
    "preprocess_layer = PreprocessLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9cbd42-3b3a-4ce5-9e8c-2a9b3327a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [18]\n",
    "# Function To Test Preprocessing Layer\n",
    "def test_preprocess_layer():\n",
    "    demo_sequence_id = example_parquet_df.index.unique()[15]\n",
    "    demo_raw_data = example_parquet_df.loc[demo_sequence_id, COLUMNS0]\n",
    "    data = preprocess_layer(demo_raw_data)\n",
    "\n",
    "    print(f'demo_raw_data shape: {demo_raw_data.shape}')\n",
    "    print(f'data shape: {data.shape}')\n",
    "    \n",
    "    return data\n",
    "    \n",
    "if IS_INTERACTIVE:\n",
    "    data = test_preprocess_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773742a8-e474-4c00-bafe-cba2ab77390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [19]\n",
    "# Train Dataset Iterator\n",
    "def get_train_dataset(X, y, batch_size=BATCH_SIZE):\n",
    "    sample_idxs = np.arange(len(X))\n",
    "    while True:\n",
    "        # Get random indices\n",
    "        random_sample_idxs = np.random.choice(sample_idxs, batch_size)\n",
    "        \n",
    "        inputs = {\n",
    "            'frames': X[random_sample_idxs],\n",
    "            'phrase': y[random_sample_idxs],\n",
    "        }\n",
    "        outputs = y[random_sample_idxs]\n",
    "        \n",
    "        yield inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33bc58-916d-419d-ba3d-9c4240cd56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [20]\n",
    "# Train Dataset\n",
    "train_dataset = get_train_dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb1db2-1fa0-454e-b3b3-212276828aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [21]\n",
    "# Training Steps Per Epoch\n",
    "TRAIN_STEPS_PER_EPOCH = math.ceil(N_TRAIN_SAMPLES / BATCH_SIZE)\n",
    "print(f'TRAIN_STEPS_PER_EPOCH: {TRAIN_STEPS_PER_EPOCH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0593b-1619-4c1e-84fb-166dae40ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [22]\n",
    "# Validation Set\n",
    "def get_val_dataset(X, y, batch_size=BATCH_SIZE):\n",
    "    offsets = np.arange(0, len(X), batch_size)\n",
    "    while True:\n",
    "        # Iterate over whole validation set\n",
    "        for offset in offsets:\n",
    "            inputs = {\n",
    "                'frames': X[offset:offset + batch_size],\n",
    "                'phrase': y[offset:offset + batch_size],\n",
    "            }\n",
    "            outputs = y[offset:offset + batch_size]\n",
    "\n",
    "            yield inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64011bf5-c9b6-4bfc-8cc5-72ec2ce8e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [23]\n",
    "# Validation Dataset\n",
    "if USE_VAL:\n",
    "    val_dataset = get_val_dataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb46b62-2d7c-485e-897f-2ac831a9a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [24]\n",
    "if USE_VAL:\n",
    "    N_VAL_STEPS_PER_EPOCH = math.ceil(N_VAL_SAMPLES / BATCH_SIZE)\n",
    "    print(f'N_VAL_STEPS_PER_EPOCH: {N_VAL_STEPS_PER_EPOCH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edded9f3-b810-4106-9095-90d0a1684dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [25]\n",
    "# Epsilon value for layer normalisation\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "\n",
    "# final embedding and transformer embedding size\n",
    "UNITS_ENCODER = 384\n",
    "UNITS_DECODER = 256\n",
    "\n",
    "# Transformer\n",
    "NUM_BLOCKS_ENCODER = 5\n",
    "NUM_BLOCKS_DECODER = 3\n",
    "NUM_HEADS = 4\n",
    "MLP_RATIO = 2\n",
    "\n",
    "# Dropout\n",
    "EMBEDDING_DROPOUT = 0.00\n",
    "MLP_DROPOUT_RATIO = 0.30\n",
    "MHA_DROPOUT_RATIO = 0.25\n",
    "CLASSIFIER_DROPOUT_RATIO = 0.10\n",
    "\n",
    "# Initializers\n",
    "INIT_HE_UNIFORM = tf.keras.initializers.he_uniform\n",
    "INIT_GLOROT_UNIFORM = tf.keras.initializers.glorot_uniform\n",
    "INIT_ZEROS = tf.keras.initializers.constant(0.0)\n",
    "# Activations\n",
    "GELU = tf.keras.activations.gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c578be-3c24-405f-af0c-e182762c2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [26]\n",
    "# Embeds a landmark using fully connected layers\n",
    "class LandmarkEmbedding(tf.keras.Model):\n",
    "    def __init__(self, units, name):\n",
    "        super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n",
    "        self.units = units\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Embedding for missing landmark in frame, initialized with zeros\n",
    "        self.empty_embedding = self.add_weight(\n",
    "            name=f'{self.name}_empty_embedding',\n",
    "            shape=[self.units],\n",
    "            initializer=INIT_ZEROS,\n",
    "        )\n",
    "        # Embedding\n",
    "        self.dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM, activation=GELU),\n",
    "            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "        ], name=f'{self.name}_dense')\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.where(\n",
    "            # Checks whether landmark is missing in frame\n",
    "            tf.reduce_sum(x, axis=2, keepdims=True) == 0,\n",
    "            # If so, the empty embedding is used\n",
    "            self.empty_embedding,\n",
    "            # Otherwise the landmark data is embedded\n",
    "            self.dense(x),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67bdbb5-9e4c-4e3f-8d6d-0a2da14c7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [27]\n",
    "# Creates embedding for each frame\n",
    "class Embedding(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.supports_masking = True\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Positional embedding for each frame index\n",
    "        self.positional_embedding = tf.Variable(\n",
    "            initial_value=tf.zeros([N_TARGET_FRAMES, UNITS_ENCODER], dtype=tf.float32),\n",
    "            trainable=True,\n",
    "            name='embedding_positional_encoder',\n",
    "        )\n",
    "        # Embedding layer for Landmarks\n",
    "        self.dominant_hand_embedding = LandmarkEmbedding(UNITS_ENCODER, 'dominant_hand')\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # Normalize\n",
    "        x = tf.where(\n",
    "                tf.math.equal(x, 0.0),\n",
    "                0.0,\n",
    "                (x - MEANS) / STDS,\n",
    "            )\n",
    "        # Dominant Hand\n",
    "        x = self.dominant_hand_embedding(x)\n",
    "        # Add Positional Encoding\n",
    "        x = x + self.positional_embedding\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e3ef3-2d62-4db2-8b59-0f6409b1412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [28]\n",
    "# based on: https://stackoverflow.com/questions/67342988/verifying-the-implementation-of-multihead-attention-in-transformer\n",
    "# replaced softmax with softmax layer to support masked softmax\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, dropout, d_out=None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Number of Units in Model\n",
    "        self.d_model = d_model\n",
    "        # Number of Attention Heads\n",
    "        self.n_heads = n_heads\n",
    "        # Number of Units in Intermediate Layers\n",
    "        self.depth = d_model // 2\n",
    "        # Scaling Factor Of Values\n",
    "        self.scale = 1.0 / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        # Learnable Projections to Depth\n",
    "        self.wq = self.fused_mha(self.depth)\n",
    "        self.wk = self.fused_mha(self.depth)\n",
    "        self.wv = self.fused_mha(self.depth)\n",
    "        # Output Projection\n",
    "        self.wo = tf.keras.layers.Dense(d_model if d_out is None else d_out, use_bias=False)\n",
    "        # Softmax Activation Which Supports Masking\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "        # Reshaping Of Multiple Attention heads to Single Value\n",
    "        self.reshape = tf.keras.Sequential([\n",
    "            # [attention heads, number of frames, d_model] → [number of frames, n_heads, d_model // n_heads]\n",
    "            tf.keras.layers.Permute([2, 1, 3]),\n",
    "            # [number of frames, attention heads, d_model] → [number of frames, d_model]\n",
    "            tf.keras.layers.Reshape([N_TARGET_FRAMES, self.depth]),\n",
    "        ])\n",
    "        # Output Dropout\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    # Single dense layer for all attention heads\n",
    "    def fused_mha(self, dim):\n",
    "        return tf.keras.Sequential([\n",
    "            # Single dense layer\n",
    "            tf.keras.layers.Dense(dim, use_bias=False),\n",
    "            # Reshape to [number of frames, number of attention head, depth]\n",
    "            tf.keras.layers.Reshape([N_TARGET_FRAMES, self.n_heads, dim // self.n_heads]),\n",
    "            # Permutate to [number of attention heads, number of frames, depth]\n",
    "            tf.keras.layers.Permute([2, 1, 3]),\n",
    "        ])\n",
    "        \n",
    "    def call(self, q, k, v, attention_mask=None, training=False):\n",
    "        # Projections to attention heads\n",
    "        Q = self.wq(q)\n",
    "        K = self.wk(k)\n",
    "        V = self.wv(v)\n",
    "        # Matrix multiply QxK to acquire attention scores\n",
    "        x = tf.matmul(Q, K, transpose_b=True) * self.scale\n",
    "        # Softmax attention scores and Multiply with Values\n",
    "        x = self.softmax(x, mask=attention_mask) @ V\n",
    "        # Reshape to flatten attention heads\n",
    "        x = self.reshape(x)\n",
    "        # Output projection\n",
    "        x = self.wo(x)\n",
    "        # Dropout\n",
    "        x = self.do(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bbe72e-7548-45e8-936e-a22c9568871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [29]\n",
    "# Encoder based on multiple transformer blocks\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, num_blocks):\n",
    "        super(Encoder, self).__init__(name='encoder')\n",
    "        self.num_blocks = num_blocks\n",
    "        self.supports_masking = True\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.ln_1s = []\n",
    "        self.mhas = []\n",
    "        self.ln_2s = []\n",
    "        self.mlps = []\n",
    "        # Make Transformer Blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            # First Layer Normalisation\n",
    "            self.ln_1s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n",
    "            # Multi Head Attention\n",
    "            self.mhas.append(MultiHeadAttention(UNITS_ENCODER, NUM_HEADS, MHA_DROPOUT_RATIO))\n",
    "            # Second Layer Normalisation\n",
    "            self.ln_2s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n",
    "            # Multi Layer Perception\n",
    "            self.mlps.append(tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(UNITS_ENCODER * MLP_RATIO, activation=GELU, kernel_initializer=INIT_GLOROT_UNIFORM, use_bias=False),\n",
    "                tf.keras.layers.Dropout(MLP_DROPOUT_RATIO),\n",
    "                tf.keras.layers.Dense(UNITS_ENCODER, kernel_initializer=INIT_HE_UNIFORM, use_bias=False),\n",
    "            ]))\n",
    "            # Optional Projection to Decoder Dimension\n",
    "            if UNITS_ENCODER != UNITS_DECODER:\n",
    "                self.dense_out = tf.keras.layers.Dense(UNITS_DECODER, kernel_initializer=INIT_GLOROT_UNIFORM, use_bias=False)\n",
    "                self.apply_dense_out = True\n",
    "            else:\n",
    "                self.apply_dense_out = False\n",
    "                \n",
    "    def get_attention_mask(self, x_inp):\n",
    "        # Attention Mask\n",
    "        attention_mask = tf.math.count_nonzero(x_inp, axis=[2], keepdims=True, dtype=tf.int32)\n",
    "        attention_mask = tf.math.count_nonzero(attention_mask, axis=[2], keepdims=False)\n",
    "        attention_mask = tf.expand_dims(attention_mask, axis=1)\n",
    "        attention_mask = tf.expand_dims(attention_mask, axis=1)\n",
    "        return attention_mask\n",
    "        \n",
    "    def call(self, x, x_inp, training=False):\n",
    "        # Attention mask to ignore missing frames\n",
    "        attention_mask = self.get_attention_mask(x_inp)\n",
    "        # Iterate input over transformer blocks\n",
    "        for ln_1, mha, ln_2, mlp in zip(self.ln_1s, self.mhas, self.ln_2s, self.mlps):\n",
    "            x = ln_1(x + mha(x, x, x, attention_mask=attention_mask))\n",
    "            x = ln_2(x + mlp(x))\n",
    "            \n",
    "        # Optional Projection to Decoder Dimension\n",
    "        if self.apply_dense_out:\n",
    "            x = self.dense_out(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ceee6-bb62-4058-a01b-e55f8b860a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [30]\n",
    "# Decoder based on multiple transformer blocks\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, num_blocks):\n",
    "        super(Decoder, self).__init__(name='decoder')\n",
    "        self.num_blocks = num_blocks\n",
    "        self.supports_masking = True\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Causal Mask Batch Size 1\n",
    "        self.causal_mask = self.get_causal_attention_mask()\n",
    "        # Positional Embedding, initialized with zeros\n",
    "        self.positional_embedding = tf.Variable(\n",
    "            initial_value=tf.zeros([N_TARGET_FRAMES, UNITS_DECODER], dtype=tf.float32),\n",
    "            trainable=True,\n",
    "            name='embedding_positional_encoder',\n",
    "        )\n",
    "        # Character Embedding\n",
    "        self.char_emb = tf.keras.layers.Embedding(N_UNIQUE_CHARACTERS, UNITS_DECODER, embeddings_initializer=INIT_ZEROS)\n",
    "        # Positional Encoder MHA\n",
    "        self.pos_emb_mha = MultiHeadAttention(UNITS_DECODER, NUM_HEADS, MHA_DROPOUT_RATIO)\n",
    "        self.pos_emb_ln = tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "        # First Layer Normalisation\n",
    "        self.ln_1s = []\n",
    "        self.mhas = []\n",
    "        self.ln_2s = []\n",
    "        self.mlps = []\n",
    "        # Make Transformer Blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            # First Layer Normalisation\n",
    "            self.ln_1s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n",
    "            # Multi Head Attention\n",
    "            self.mhas.append(MultiHeadAttention(UNITS_DECODER, NUM_HEADS, MHA_DROPOUT_RATIO))\n",
    "            # Second Layer Normalisation\n",
    "            self.ln_2s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n",
    "            # Multi Layer Perception\n",
    "            self.mlps.append(tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(UNITS_DECODER * MLP_RATIO, activation=GELU, kernel_initializer=INIT_GLOROT_UNIFORM, use_bias=False),\n",
    "                tf.keras.layers.Dropout(MLP_DROPOUT_RATIO),\n",
    "                tf.keras.layers.Dense(UNITS_DECODER, kernel_initializer=INIT_HE_UNIFORM, use_bias=False),\n",
    "            ]))\n",
    "            \n",
    "    def get_causal_attention_mask(self):\n",
    "        i = tf.range(N_TARGET_FRAMES)[:, tf.newaxis]\n",
    "        j = tf.range(N_TARGET_FRAMES)\n",
    "        mask = tf.cast(i >= j, dtype=tf.int32)\n",
    "        mask = tf.reshape(mask, (1, N_TARGET_FRAMES, N_TARGET_FRAMES))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(1, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        mask = tf.tile(mask, mult)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return mask\n",
    "    \n",
    "    def get_attention_mask(self, x_inp):\n",
    "        # Attention Mask\n",
    "        attention_mask = tf.math.count_nonzero(x_inp, axis=[2], keepdims=True, dtype=tf.int32)\n",
    "        attention_mask = tf.math.count_nonzero(attention_mask, axis=[2], keepdims=False)\n",
    "        attention_mask = tf.expand_dims(attention_mask, axis=1)\n",
    "        attention_mask = tf.expand_dims(attention_mask, axis=1)\n",
    "        return attention_mask\n",
    "        \n",
    "    def call(self, encoder_outputs, phrase, x_inp, training=False):\n",
    "        # Batch Size\n",
    "        B = tf.shape(encoder_outputs)[0]\n",
    "        # Cast to INT32\n",
    "        phrase = tf.cast(phrase, tf.int32)\n",
    "        # Prepend SOS Token\n",
    "        phrase = tf.pad(phrase, [[0, 0], [1, 0]], constant_values=SOS_TOKEN, name='prepend_sos_token')\n",
    "        # Pad With PAD Token\n",
    "        phrase = tf.pad(phrase, [[0, 0], [0, N_TARGET_FRAMES-MAX_PHRASE_LENGTH-1]], constant_values=PAD_TOKEN, name='append_pad_token')\n",
    "        # Positional Embedding\n",
    "        x = self.positional_embedding + self.char_emb(phrase)\n",
    "        # Causal Attention\n",
    "        x = self.pos_emb_ln(x + self.pos_emb_mha(x, x, x, attention_mask=self.causal_mask))\n",
    "        # Attention mask to ignore missing frames\n",
    "        attention_mask = self.get_attention_mask(x_inp)\n",
    "        # Iterate input over transformer blocks\n",
    "        for ln_1, mha, ln_2, mlp in zip(self.ln_1s, self.mhas, self.ln_2s, self.mlps):\n",
    "            x = ln_1(x + mha(x, encoder_outputs, encoder_outputs, attention_mask=attention_mask))\n",
    "            x = ln_2(x + mlp(x))\n",
    "        # Slice 31 Characters\n",
    "        x = tf.slice(x, [0, 0, 0], [-1, MAX_PHRASE_LENGTH, -1])\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c0289-c04f-40c2-a678-c5ca23d0fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [31]\n",
    "# Causal Attention to make decoder not attent to future characters which it needs to predict\n",
    "def get_causal_attention_mask(B):\n",
    "    i = tf.range(N_TARGET_FRAMES)[:, tf.newaxis]\n",
    "    j = tf.range(N_TARGET_FRAMES)\n",
    "    mask = tf.cast(i >= j, dtype=tf.int32)\n",
    "    mask = tf.reshape(mask, (1, N_TARGET_FRAMES, N_TARGET_FRAMES))\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(B, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "        axis=0,\n",
    "    )\n",
    "    mask = tf.tile(mask, mult)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return mask\n",
    "\n",
    "get_causal_attention_mask(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4684ddd-00d1-490f-93de-886d8591baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [32]\n",
    "# TopK accuracy for multi dimensional output\n",
    "class TopKAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, k, **kwargs):\n",
    "        super(TopKAccuracy, self).__init__(name=f'top{k}acc', **kwargs)\n",
    "        self.top_k_acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=k)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.reshape(y_true, [-1])\n",
    "        y_pred = tf.reshape(y_pred, [-1, N_UNIQUE_CHARACTERS])\n",
    "        character_idxs = tf.where(y_true < N_UNIQUE_CHARACTERS0)\n",
    "        y_true = tf.gather(y_true, character_idxs, axis=0)\n",
    "        y_pred = tf.gather(y_pred, character_idxs, axis=0)\n",
    "        self.top_k_acc.update_state(y_true, y_pred)\n",
    "\n",
    "    def result(self):\n",
    "        return self.top_k_acc.result()\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.top_k_acc.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb824cf2-ff59-461d-9799-3622a3f8073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [33]\n",
    "# Create Initial Loss Weights All Set To 1\n",
    "loss_weights = np.ones(N_UNIQUE_CHARACTERS, dtype=np.float32)\n",
    "# Set Loss Weight Of Pad Token To 0\n",
    "loss_weights[PAD_TOKEN] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716f8f2-8c0b-4c6a-86a1-28518310148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [34]\n",
    "# source:: https://stackoverflow.com/questions/60689185/label-smoothing-for-sparse-categorical-crossentropy\n",
    "def scce_with_ls(y_true, y_pred):\n",
    "    # Filter Pad Tokens\n",
    "    idxs = tf.where(y_true != PAD_TOKEN)\n",
    "    y_true = tf.gather_nd(y_true, idxs)\n",
    "    y_pred = tf.gather_nd(y_pred, idxs)\n",
    "    # One Hot Encode Sparsely Encoded Target Sign\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_true = tf.one_hot(y_true, N_UNIQUE_CHARACTERS, axis=1)\n",
    "    # Categorical Cross Entropy with native label smoothing support\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=0.25, from_logits=True)\n",
    "    loss = tf.math.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ff771-0fc9-43bf-bea6-230266a9c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [35]\n",
    "def get_model():\n",
    "    # Inputs\n",
    "    frames_inp = tf.keras.layers.Input([N_TARGET_FRAMES, N_COLS], dtype=tf.float32, name='frames')\n",
    "    phrase_inp = tf.keras.layers.Input([MAX_PHRASE_LENGTH], dtype=tf.int32, name='phrase')\n",
    "    # Frames\n",
    "    x = frames_inp\n",
    "\n",
    "    # Masking\n",
    "    x = tf.keras.layers.Masking(mask_value=0.0, input_shape=(N_TARGET_FRAMES, N_COLS))(x)\n",
    "    \n",
    "    # Embedding\n",
    "    x = Embedding()(x)\n",
    "    \n",
    "    # Encoder Transformer Blocks\n",
    "    x = Encoder(NUM_BLOCKS_ENCODER)(x, frames_inp)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Decoder(NUM_BLOCKS_DECODER)(x, phrase_inp, frames_inp)\n",
    "    \n",
    "    # Classifier\n",
    "    x = tf.keras.Sequential([\n",
    "        # Dropout\n",
    "        tf.keras.layers.Dropout(CLASSIFIER_DROPOUT_RATIO),\n",
    "        # Output Neurons\n",
    "        tf.keras.layers.Dense(N_UNIQUE_CHARACTERS, activation=tf.keras.activations.linear, kernel_initializer=INIT_HE_UNIFORM, use_bias=False),\n",
    "    ], name='classifier')(x)\n",
    "    \n",
    "    outputs = x\n",
    "    \n",
    "    # Create TensorFlow Model\n",
    "    model = tf.keras.models.Model(inputs=[frames_inp, phrase_inp], outputs=outputs)\n",
    "    \n",
    "    # Categorical Cross Entropy Loss With Label Smoothing\n",
    "    loss = scce_with_ls\n",
    "    \n",
    "    # Adam Optimizer\n",
    "    optimizer = tfa.optimizers.RectifiedAdam(sma_threshold=4)\n",
    "    optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n",
    "\n",
    "    # TopK Metrics\n",
    "    metrics = [\n",
    "        TopKAccuracy(1),\n",
    "        TopKAccuracy(5),\n",
    "    ]\n",
    "    \n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics,\n",
    "        loss_weights=loss_weights,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d60c054-14b8-480e-ae5c-849a72cb5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [36]\n",
    "# Input data\n",
    "for k, v in X_batch.items():\n",
    "    print(f'{k}: {v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4aba7-0cca-47aa-bfef-6649998ba74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [37]\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7acd1-af81-4d99-acf3-65f851c6820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [38]\n",
    "# Plot model summary\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c6d70-3fa3-4035-a1fb-e4b8461d8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [39]\n",
    "# Plot Model Architecture\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbad0d6-f154-4a12-a1ea-687d4459045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [40]\n",
    "def verify_correct_training_flag():\n",
    "    # Verify static output for inference\n",
    "    pred = model(X_batch_small, training=False)\n",
    "    for _ in tqdm(range(10)):\n",
    "        assert tf.reduce_min(tf.cast(pred == model(X_batch_small, training=False), tf.int8)) == 1\n",
    "\n",
    "    # Verify at least 99% varying output due to dropout during training\n",
    "    for _ in tqdm(range(10)):\n",
    "        assert tf.reduce_mean(tf.cast(pred != model(X_batch_small, training=True), tf.float32)) > 0.99\n",
    "        \n",
    "verify_correct_training_flag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08bafdd-07d8-451b-b8f2-329ebe7929fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [41]\n",
    "# Verify No NaN predictions\n",
    "def verify_no_nan_predictions():\n",
    "    y_pred = model.predict(\n",
    "        val_dataset if USE_VAL else train_dataset,\n",
    "        steps=N_VAL_STEPS_PER_EPOCH if USE_VAL else 100,\n",
    "        verbose=VERBOSE,\n",
    "    )\n",
    "\n",
    "    print(f'# NaN Values In Predictions: {np.isnan(y_pred).sum()}')\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.title(f'Logit Predictions Initialized Model')\n",
    "    pd.Series(y_pred.flatten()).plot(kind='hist', bins=128)\n",
    "    plt.xlabel('Logits')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "verify_no_nan_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff4b82-b41e-4cfa-95bb-96a035184f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [42]\n",
    "def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n",
    "    \n",
    "    if current_step < num_warmup_steps:\n",
    "        if WARMUP_METHOD == 'log':\n",
    "            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n",
    "        else:\n",
    "            return lr_max * 2 ** -(num_warmup_steps - current_step)\n",
    "    else:\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50066d10-39ce-4276-9e23-b300a3d861f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [43]\n",
    "def plot_lr_schedule(lr_schedule, epochs):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    plt.plot([None] + lr_schedule + [None])\n",
    "    # X Labels\n",
    "    x = np.arange(1, epochs + 1)\n",
    "    # x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n",
    "    x_axis_labels = [i if epochs <= 40 or i % 10 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n",
    "    plt.xlim([1, epochs])\n",
    "    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n",
    "    \n",
    "    # Increase y limit for better readability\n",
    "    plt.ylim([0, max(lr_schedule) * 1.1])\n",
    "    \n",
    "    # Title\n",
    "    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n",
    "    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n",
    "    \n",
    "    # Plot Learning Rates\n",
    "    for x, val in enumerate(lr_schedule):\n",
    "        # if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n",
    "        if epochs <= 40 or x % 10 == 0 or x is epochs - 1:\n",
    "            if x < len(lr_schedule) - 1:\n",
    "                if lr_schedule[x - 1] < val:\n",
    "                    ha = 'right'\n",
    "                else:\n",
    "                    ha = 'left'\n",
    "            elif x == 0:\n",
    "                ha = 'right'\n",
    "            else:\n",
    "                ha = 'left'\n",
    "            plt.plot(x + 1, val, 'o', color='black');\n",
    "            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n",
    "            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n",
    "    \n",
    "    plt.xlabel('Epoch', size=16, labelpad=5)\n",
    "    plt.ylabel('Learning Rate', size=16, labelpad=5)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Learning rate for encoder\n",
    "LR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\n",
    "# Plot Learning Rate Schedule\n",
    "plot_lr_schedule(LR_SCHEDULE, epochs=N_EPOCHS)\n",
    "# Learning Rate Callback\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13506fc8-c2ff-4db9-990a-259c87636a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [44]\n",
    "# Custom callback to update weight decay with learning rate\n",
    "class WeightDecayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, wd_ratio=WD_RATIO):\n",
    "        self.step_counter = 0\n",
    "        self.wd_ratio = wd_ratio\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n",
    "        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb48a4-8dfd-4244-a47b-813df29978c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [45]\n",
    "# Evaluate Initialized Model On Validation Data\n",
    "y_pred = model.evaluate(\n",
    "    val_dataset if USE_VAL else train_dataset,\n",
    "    steps=N_VAL_STEPS_PER_EPOCH if USE_VAL else TRAIN_STEPS_PER_EPOCH,\n",
    "    verbose=VERBOSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f894a-3030-4e81-97b8-616bfdb70d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [46]\n",
    "# baseline accuracy when only pad token is predicted\n",
    "if USE_VAL:\n",
    "    baseline_accuracy = np.mean(y_val == PAD_TOKEN)\n",
    "else:\n",
    "    baseline_accuracy = np.mean(y_train == PAD_TOKEN)\n",
    "print(f'Baseline Accuracy: {baseline_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100411d-382e-4cef-9c56-02bce2339827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [47]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3de16-cd52-450c-bdc2-a54825afa00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [48]\n",
    "if TRAIN_MODEL:\n",
    "    # Clear all models in GPU\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Get new fresh model\n",
    "    model = get_model()\n",
    "\n",
    "    # Sanity Check\n",
    "    model.summary()\n",
    "\n",
    "    # Actual Training\n",
    "    history = model.fit(\n",
    "        x=train_dataset,\n",
    "        steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "        epochs=N_EPOCHS,\n",
    "        # Only used for validation data since training data is a generator\n",
    "        validation_data=val_dataset if USE_VAL else None,\n",
    "        validation_steps=N_VAL_STEPS_PER_EPOCH if USE_VAL else None,\n",
    "        callbacks=[\n",
    "            lr_callback,\n",
    "            WeightDecayCallback(),\n",
    "        ],\n",
    "        verbose=VERBOSE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf4eda-36bd-4ad3-b926-3884bd8a62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [49]\n",
    "# Load Weights\n",
    "if LOAD_WEIGHTS:\n",
    "    model.load_weights('/kaggle/input/aslfr-training-python37/model.h5')\n",
    "    print(f'Successfully Loaded Pretrained Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf9c2d-7a5b-49b5-987d-b2d2660fbdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [50]\n",
    "# Save Model Weights\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7037fc5-c2db-4f9c-b38d-6696c1f10201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [51]\n",
    "# Verify Model is Loaded Correctly\n",
    "model.evaluate(\n",
    "    val_dataset if USE_VAL else train_dataset,\n",
    "    steps=N_VAL_STEPS_PER_EPOCH if USE_VAL else TRAIN_STEPS_PER_EPOCH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=VERBOSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56922fa4-1be5-4e5f-9722-0e58831a7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [52]\n",
    "# Output Predictions to string\n",
    "def outputs2phrase(outputs):\n",
    "    if outputs.ndim == 2:\n",
    "        outputs = np.argmax(outputs, axis=1)\n",
    "    \n",
    "    return ''.join([ORD2CHAR.get(s, '') for s in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9146c23-59f6-4523-a143-d5c080959344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [53]\n",
    "@tf.function(jit_compile=True)\n",
    "def predict_phrase(frames):\n",
    "    # Add Batch Dimension\n",
    "    frames = tf.expand_dims(frames, axis=0)\n",
    "    # Start Phrase\n",
    "    phrase = tf.fill([1, MAX_PHRASE_LENGTH], PAD_TOKEN)\n",
    "\n",
    "    for idx in tf.range(MAX_PHRASE_LENGTH):\n",
    "        # Cast phrase to int8\n",
    "        phrase = tf.cast(phrase, tf.int8)\n",
    "        # Predict Next Token\n",
    "        outputs = model({\n",
    "            'frames': frames,\n",
    "            'phrase': phrase,\n",
    "        })\n",
    "\n",
    "        # Add predicted token to input phrase\n",
    "        phrase = tf.cast(phrase, tf.int32)\n",
    "        phrase = tf.where(\n",
    "            tf.range(MAX_PHRASE_LENGTH) < idx + 1,\n",
    "            tf.argmax(outputs, axis=2, output_type=tf.int32),\n",
    "            phrase,\n",
    "        )\n",
    "\n",
    "    # Squeeze outputs\n",
    "    outputs = tf.squeeze(phrase, axis=0)\n",
    "    outputs = tf.one_hot(outputs, N_UNIQUE_CHARACTERS)\n",
    "\n",
    "    # Return a dictionary with the output tensor\n",
    "    return outputs\n",
    "\n",
    "    # Return a dictionary with the output tensor\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb6de9-7db0-40ad-b40c-d8ce8eb4fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [54]\n",
    "# Compute Levenshtein Distances\n",
    "def get_ld_train():\n",
    "    N = 100 if IS_INTERACTIVE else 1000\n",
    "    LD_TRAIN = []\n",
    "    for idx, (frames, phrase_true) in enumerate(zip(tqdm(X_train, total=N), y_train)):\n",
    "        # Predict Phrase and Convert to String\n",
    "        phrase_pred = predict_phrase(frames).numpy()\n",
    "        phrase_pred = outputs2phrase(phrase_pred)\n",
    "        # True Phrase Ordinal to String\n",
    "        phrase_true = outputs2phrase(phrase_true)\n",
    "        # Add Levenshtein Distance\n",
    "        LD_TRAIN.append({\n",
    "            'phrase_true': phrase_true,\n",
    "            'phrase_true_len': len(phrase_true),\n",
    "            'phrase_pred': phrase_pred,\n",
    "            'levenshtein_distance': levenshtein(phrase_pred, phrase_true),\n",
    "        })\n",
    "        # Take subset in interactive mode\n",
    "        if idx == N:\n",
    "            break\n",
    "            \n",
    "    # Convert to DataFrame\n",
    "    LD_TRAIN_DF = pd.DataFrame(LD_TRAIN)\n",
    "    \n",
    "    return LD_TRAIN_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299e3d4d-759e-4170-8a26-461da98d2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [55]\n",
    "LD_TRAIN_DF = get_ld_train()\n",
    "\n",
    "# Display Errors\n",
    "display(LD_TRAIN_DF.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f9d08-2d00-4a1c-af6d-e510c99af66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [56]\n",
    "# Value Counts\n",
    "LD_TRAIN_VC = dict([(i, 0) for i in range(LD_TRAIN_DF['levenshtein_distance'].max()+1)])\n",
    "for ld in LD_TRAIN_DF['levenshtein_distance']:\n",
    "    LD_TRAIN_VC[ld] += 1\n",
    "\n",
    "# Evaluation Metric\n",
    "N = LD_TRAIN_DF['phrase_true_len'].sum()\n",
    "D = LD_TRAIN_DF['levenshtein_distance'].sum()\n",
    "nld = (N - D) / N\n",
    "\n",
    "LD_TRAIN_VC = dict([(i, 0) for i in range(LD_TRAIN_DF['levenshtein_distance'].max()+1)])\n",
    "for ld in LD_TRAIN_DF['levenshtein_distance']:\n",
    "    LD_TRAIN_VC[ld] += 1\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "pd.Series(LD_TRAIN_VC).plot(kind='bar', width=1)\n",
    "plt.title(f'Train Levenshtein Distance Distribution | Mean: {LD_TRAIN_DF.levenshtein_distance.mean():.4f}, NLD: {nld:.3f}')\n",
    "plt.xlabel('Levenshtein Distance')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.xlim(-0.50, LD_TRAIN_DF.levenshtein_distance.max()+0.50)\n",
    "plt.grid(axis='y')\n",
    "plt.savefig('temp.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff669752-bf1a-4491-b402-497fbf467d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [57]\n",
    "# Compute Levenshtein Distances\n",
    "def get_ld_val():\n",
    "    N = 100 if IS_INTERACTIVE else 1000\n",
    "    LD_VAL = []\n",
    "    for idx, (frames, phrase_true) in enumerate(zip(tqdm(X_val, total=N), y_val)):\n",
    "        # Predict Phrase and Convert to String\n",
    "        phrase_pred = predict_phrase(frames).numpy()\n",
    "        phrase_pred = outputs2phrase(phrase_pred)\n",
    "        # True Phrase Ordinal to String\n",
    "        phrase_true = outputs2phrase(phrase_true)\n",
    "        # Add Levenshtein Distance\n",
    "        LD_VAL.append({\n",
    "            'phrase_true': phrase_true,\n",
    "            'phrase_true_len': len(phrase_true),\n",
    "            'phrase_pred': phrase_pred,\n",
    "            'levenshtein_distance': levenshtein(phrase_pred, phrase_true),\n",
    "        })\n",
    "        # Take subset in interactive mode\n",
    "        if idx == N:\n",
    "            break\n",
    "            \n",
    "    # Convert to DataFrame\n",
    "    LD_VAL_DF = pd.DataFrame(LD_VAL)\n",
    "    \n",
    "    return LD_VAL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e305b2-8e2e-4b20-a122-a1496777f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [58]\n",
    "if USE_VAL:\n",
    "    LD_VAL_DF = get_ld_val()\n",
    "\n",
    "    # Display Errors\n",
    "    display(LD_VAL_DF.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579aec92-c52b-4c0e-9cc7-9aa75ff41329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [59]\n",
    "# Value Counts\n",
    "if USE_VAL:\n",
    "    # Evaluation Metric\n",
    "    N = LD_VAL_DF['phrase_true_len'].sum()\n",
    "    D = LD_VAL_DF['levenshtein_distance'].sum()\n",
    "    nld = (N - D) / N\n",
    "    \n",
    "    LD_VAL_VC = dict([(i, 0) for i in range(LD_VAL_DF['levenshtein_distance'].max()+1)])\n",
    "    for ld in LD_VAL_DF['levenshtein_distance']:\n",
    "        LD_VAL_VC[ld] += 1\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    pd.Series(LD_VAL_VC).plot(kind='bar', width=1)\n",
    "    plt.title(f'Validation Levenshtein Distance Distribution | Mean: {LD_VAL_DF.levenshtein_distance.mean():.4f}, NLD: {nld:.3f}')\n",
    "    plt.xlabel('Levenshtein Distance')\n",
    "    plt.ylabel('Sample Count')\n",
    "    plt.xlim(-0.50, LD_VAL_DF.levenshtein_distance.max() + 0.50)\n",
    "    plt.grid(axis='y')\n",
    "    plt.savefig('temp.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ba2f8-5de9-4ee5-80db-15458d659bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [60]\n",
    "def plot_history_metric(metric, f_best=np.argmax, ylim=None, yscale=None, yticks=None):\n",
    "    # Only plot when training\n",
    "    if not TRAIN_MODEL:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    values = history.history[metric]\n",
    "    N_EPOCHS = len(values)\n",
    "    val = 'val' in ''.join(history.history.keys())\n",
    "    # Epoch Ticks\n",
    "    if N_EPOCHS <= 20:\n",
    "        x = np.arange(1, N_EPOCHS + 1)\n",
    "    else:\n",
    "        # x = [1, 5] + [10 + 5 * idx for idx in range((N_EPOCHS - 10) // 5 + 1)]\n",
    "        x = [1, 10] + [20 + 10 * idx for idx in range((N_EPOCHS - 20) // 10 + 1)]\n",
    "\n",
    "    x_ticks = np.arange(1, N_EPOCHS + 1)\n",
    "\n",
    "    # Validation\n",
    "    if val:\n",
    "        val_values = history.history[f'val_{metric}']\n",
    "        val_argmin = f_best(val_values)\n",
    "        plt.plot(x_ticks, val_values, label=f'val')\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(x_ticks, values, label=f'train')\n",
    "    argmin = f_best(values)\n",
    "    plt.scatter(argmin + 1, values[argmin], color='red', s=75, marker='o', label=f'train_best')\n",
    "    if val:\n",
    "        plt.scatter(val_argmin + 1, val_values[val_argmin], color='purple', s=75, marker='o', label=f'val_best')\n",
    "\n",
    "    plt.title(f'Model {metric}', fontsize=24, pad=10)\n",
    "    plt.ylabel(metric, fontsize=20, labelpad=10)\n",
    "\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "        \n",
    "    if yticks is not None:\n",
    "        plt.yticks(yticks, fontsize=16)\n",
    "\n",
    "    plt.xlabel('epoch', fontsize=20, labelpad=10)        \n",
    "    plt.tick_params(axis='x', labelsize=8)\n",
    "    plt.xticks(x, fontsize=16) # set tick step to 1 and let x axis start at 1\n",
    "    plt.yticks(fontsize=16)\n",
    "    \n",
    "    plt.legend(prop={'size': 10})\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d479d3-7c1c-46d7-ac5d-a6bb2d53d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [61]\n",
    "plot_history_metric('loss', f_best=np.argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881f76e-1ba4-409a-ae6b-9d2818188614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [62]\n",
    "plot_history_metric('top1acc', ylim=[0, 1], yticks=np.arange(0.0, 1.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7a60d-ebf9-4102-bac1-6adbc4d0a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [63]\n",
    "plot_history_metric('top5acc', ylim=[0, 1], yticks=np.arange(0.0, 1.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3de4dc-b6e3-4eb1-8206-466002d97bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [64]\n",
    "# Model Layer Names\n",
    "for l in model.layers:\n",
    "    print(l.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d3449-c96b-45e1-8e5c-25a308f09f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [65]\n",
    "# TFLite model for submission\n",
    "class TFLiteModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "\n",
    "        # Load the feature generation and main models\n",
    "        self.preprocess_layer = preprocess_layer\n",
    "        self.model = model\n",
    "    \n",
    "    @tf.function(jit_compile=True)\n",
    "    def encoder(self, x, frames_inp):\n",
    "        x = self.model.get_layer('embedding')(x)\n",
    "        x = self.model.get_layer('encoder')(x, frames_inp)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    @tf.function(jit_compile=True)\n",
    "    def decoder(self, x, phrase_inp, frames_inp):\n",
    "        x = self.model.get_layer('decoder')(x, phrase_inp, frames_inp)\n",
    "        x = self.model.get_layer('classifier')(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, N_COLS0], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs):\n",
    "        # Number Of Input Frames\n",
    "        N_INPUT_FRAMES = tf.shape(inputs)[0]\n",
    "        # Preprocess Data\n",
    "        frames_inp = self.preprocess_layer(inputs)        \n",
    "        # Add Batch Dimension\n",
    "        frames_inp = tf.expand_dims(frames_inp, axis=0)\n",
    "        # Get Encoding\n",
    "        encoding = self.encoder(frames_inp, frames_inp)\n",
    "        # Make Prediction\n",
    "        phrase = tf.fill([1, MAX_PHRASE_LENGTH], PAD_TOKEN)\n",
    "        # Predict One Token At A Time\n",
    "        stop = False\n",
    "        for idx in tf.range(MAX_PHRASE_LENGTH):\n",
    "            # Cast phrase to int8\n",
    "            phrase = tf.cast(phrase, tf.int8)\n",
    "            # If EOS token is predicted, stop predicting\n",
    "            outputs = tf.cond(\n",
    "                stop,\n",
    "                lambda: tf.one_hot(tf.cast(phrase, tf.int32), N_UNIQUE_CHARACTERS),\n",
    "                lambda: self.decoder(encoding, phrase, frames_inp)\n",
    "            )\n",
    "            # Add predicted token to input phrase\n",
    "            phrase = tf.cast(phrase, tf.int32)\n",
    "            # Replace PAD token with predicted token up to idx\n",
    "            phrase = tf.where(\n",
    "                tf.range(MAX_PHRASE_LENGTH) < idx + 1,\n",
    "                tf.argmax(outputs, axis=2, output_type=tf.int32),\n",
    "                phrase,\n",
    "            )\n",
    "            # Predicted Token\n",
    "            predicted_token = phrase[0, idx]\n",
    "            # If EOS (End Of Sentence) token is predicted stop\n",
    "            if not stop:\n",
    "                stop = predicted_token == EOS_TOKEN\n",
    "            \n",
    "        # Squeeze outputs\n",
    "        outputs = tf.squeeze(phrase, axis=0)\n",
    "        outputs = tf.one_hot(outputs, N_UNIQUE_CHARACTERS)\n",
    "            \n",
    "        # Return a dictionary with the output tensor\n",
    "        return {'outputs': outputs }\n",
    "\n",
    "# Define TF Lite Model\n",
    "tflite_keras_model = TFLiteModel(model)\n",
    "\n",
    "# Sanity Check\n",
    "# demo_sequence_id = 1816796431\n",
    "demo_sequence_id = example_parquet_df.index.unique()[0]\n",
    "demo_raw_data = example_parquet_df.loc[demo_sequence_id, COLUMNS0].values\n",
    "demo_phrase_true = train_sequence_id.loc[demo_sequence_id, 'phrase']\n",
    "print(f'demo_raw_data shape: {demo_raw_data.shape}, dtype: {demo_raw_data.dtype}')\n",
    "demo_output = tflite_keras_model(demo_raw_data)['outputs'].numpy()\n",
    "print(f'demo_output shape: {demo_output.shape}, dtype: {demo_output.dtype}')\n",
    "print(f'demo_outputs phrase decoded: {outputs2phrase(demo_output)}')\n",
    "print(f'phrase true: {demo_phrase_true}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356dd240-6158-410b-a4f4-6aab9f4c12d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [66]\n",
    "# Create Model Converter\n",
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflite_keras_model)\n",
    "# Convert Model\n",
    "tflite_model = keras_model_converter.convert()\n",
    "# Write Model\n",
    "with open('/kaggle/working/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6994bd-1470-4eca-aab8-cc5737a41541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [67]\n",
    "# Add selected_columns json to only select specific columns from input frames\n",
    "# with open('inference_args.json', 'w') as f:\n",
    "#      json.dump({ 'selected_columns': COLUMNS0.tolist() }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0daaf23-7792-4a1a-b3b5-a89946b39c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [calv, kaggle] [68]\n",
    "# Zip Model\n",
    "# !zip submission.zip /kaggle/working/model.tflite /kaggle/working/inference_args.json"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m109"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
